# Generative-AI-at-a-Glance-LLM-Workflow--Explained-Simply


𝟭. 𝗟𝗟𝗠 (𝗟𝗮𝗿𝗴𝗲 𝗟𝗮𝗻𝗴𝘂𝗮𝗴𝗲 𝗠𝗼𝗱𝗲𝗹) 

→ Helps computers understand and write human-like text 
→ Examples: GPT-4, Claude, Gemini 
→ Used in: Chatbots, coding tools, content generation

𝟮. 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗲𝗿𝘀 

→ The tech behind all modern AI models 
→ Let models understand meaning, context, and order of words 
→ Examples: BERT, GPT

𝟯. 𝗣𝗿𝗼𝗺𝗽𝘁 𝗘𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴 

→ Writing better instructions to get better AI answers 
→ Includes system prompts, step-by-step prompts, and safety rules

𝟰. 𝗙𝗶𝗻𝗲-𝗧𝘂𝗻𝗶𝗻𝗴 

→ Training an AI model on your data 
→ Helps tailor it for specific tasks like legal, medical, or financial use cases

𝟱. 𝗘𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴𝘀 

→ A way for AI to understand meaning and relationships between words or documents 
→ Used in search engines and recommendation systems

𝟲. 𝗥𝗔𝗚 (𝗥𝗲𝘁𝗿𝗶𝗲𝘃𝗮𝗹-𝗔𝘂𝗴𝗺𝗲𝗻𝘁𝗲𝗱 𝗚𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗼𝗻) 

→ Combines AI with a database or document store 
→ Helps AI give more accurate, fact-based answers

𝟳. 𝗧𝗼𝗸𝗲𝗻𝘀 

→ The chunks of text AI reads and writes 
→ Managing them controls cost and performance

*. 𝗖𝗵𝗮𝗶𝗻-𝗼𝗳-𝗧𝗵𝗼𝘂𝗴𝗵𝘁 

→ AI explains its answer step-by-step 
→ Helps with complex reasoning tasks

𝗪𝗵𝗮𝘁’𝘀  𝗡𝗲𝘅𝘁? 

→ Multimodal AI (text, images, audio together) 
→ Smaller, faster models 
→ Safer, ethical AI (Constitutional AI) 
→ Agentic AI (autonomous, task-completing agents)




What is an LLM?
An LLM, or Large Language Model, is a type of neural network specifically
designed to process and generate human-like text. These models are built on
massive datasets, intricate architectures, and intensive training processes.
Here’s a simple formula to understand LLMs:
LLM = DATA + ARCHITECTURE + TRAINING

Data
LLMs are trained on petabytes of data. To give you an idea:
1 GB can contain around 178 million words.
1 PB (petabyte) equals 1 million GBS

This immense amount of data comes from various sources:
Books
Transcripts
Web scraping

Architecture
The architecture of LLMs, like GPT-4, is typically based on transformers.
Transformers are a type of model architecture that significantly advanced
the field by reducing training time and improving performance.
Training
Training involves feeding the data into the architecture and adjusting the
model based on its performance in predicting and generating text.
How Does an LLM Work?
At its core, an LLM is a neural network trained on vast amounts of data to
recognize patterns in text. Here’s a breakdown of its key components:
1. Tokenization: The text is broken down into individual tokens. Tokens can
be single words or parts of words. For example:
Single token: “Hello”
Multi-token: “Summarization” -> “sum”, “mar”, “ization”
2. Embedding: These tokens are then converted into vectors (1D
representations). This step helps the model understand the context and
relationships between words.
3. Transformers: Utilizing multi-head attention mechanisms, transformers
process these embeddings to predict and generate text. Transformers use
metrics like perplexity to measure how well the model predicts the next
word in a sequence.
Applications of LLMs
LLMs are incredibly versatile and can be used for:
Summarization: Condensing long texts into shorter summaries.
Text Generation: Creating coherent and contextually relevant text.
Creative Writing: Assisting in generating stories, poems, and more.
Q&A: Providing answers to questions based on the data they were trained
on.
Programming: Helping with code






→ When AI gives wrong or made-up answers 
→ Can be fixed with fact-checking and better prompts
